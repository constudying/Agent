# å¦‚ä½•è§‚å¯Ÿç¼–ç å™¨å’Œè§£ç å™¨çš„æ³¨æ„åŠ›å›¾ - å¿«é€ŸæŒ‡å—

## ğŸ“Š ä¸€åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹

```python
from agent.models.transformer import Transformer, AttentionVisualizer
import torch

# 1. è·å–æ³¨æ„åŠ›æƒé‡
model = Transformer(...)
model.enable_attention_storage()

with torch.no_grad():
    output, attn = model(enc, dec, return_attention_weights=True)

model.disable_attention_storage()

# 2. åˆ›å»ºå¯è§†åŒ–å™¨
visualizer = AttentionVisualizer()

# 3. æŸ¥çœ‹ç¼–ç å™¨æ³¨æ„åŠ›
visualizer.plot_attention_heatmap(
    attn['encoder'][0]['self_attention'],
    title='ç¼–ç å™¨ç¬¬0å±‚è‡ªæ³¨æ„åŠ›',
    save_path='encoder_attn.png'
)

# 4. æŸ¥çœ‹è§£ç å™¨äº¤å‰æ³¨æ„åŠ›
visualizer.plot_attention_heatmap(
    attn['decoder'][0]['cross_attention'],
    title='è§£ç å™¨ç¬¬0å±‚äº¤å‰æ³¨æ„åŠ›',
    save_path='decoder_cross_attn.png'
)
```

## ğŸ¯ æ³¨æ„åŠ›ç±»å‹è¯´æ˜

### 1. ç¼–ç å™¨è‡ªæ³¨æ„åŠ› (Encoder Self-Attention)

**ä½œç”¨**: ç¼–ç å™¨å†…éƒ¨ï¼Œæ¯ä¸ªä½ç½®å…³æ³¨å…¶ä»–ä½ç½®

```python
# æŸ¥çœ‹ç¼–ç å™¨ç¬¬0å±‚
attn['encoder'][0]['self_attention']
# å½¢çŠ¶: (batch_size, num_heads, seq_len, seq_len)

# å¯è§†åŒ–
visualizer.plot_attention_heatmap(
    attn['encoder'][0]['self_attention'],
    title='ç¼–ç å™¨è‡ªæ³¨æ„åŠ›'
)
```

**å¦‚ä½•è§£è¯»**:

- **è¡Œ**: Queryä½ç½®ï¼ˆé—®"æˆ‘åº”è¯¥å…³æ³¨è°ï¼Ÿ"ï¼‰
- **åˆ—**: Keyä½ç½®ï¼ˆè¢«å…³æ³¨çš„ä½ç½®ï¼‰
- **äº®ç‚¹**: è¡¨ç¤ºå¼ºæ³¨æ„åŠ›
- **æš—ç‚¹**: è¡¨ç¤ºå¼±æ³¨æ„åŠ›

**ä¾‹å­**:

- å¦‚æœä½ç½®5å¯¹ä½ç½®3çš„å€¼å¾ˆäº® â†’ ä½ç½®5åœ¨å…³æ³¨ä½ç½®3
- å¯¹è§’çº¿äº® â†’ ä½ç½®å…³æ³¨è‡ªå·±
- æŸè¡Œå…¨äº® â†’ è¯¥ä½ç½®å…³æ³¨æ‰€æœ‰ä½ç½®

### 2. è§£ç å™¨äº¤å‰æ³¨æ„åŠ› (Decoder Cross-Attention)

**ä½œç”¨**: è§£ç å™¨å…³æ³¨ç¼–ç å™¨ï¼Œè¿™æ˜¯ä¿¡æ¯ä»è¾“å…¥æµå‘è¾“å‡ºçš„å…³é”®

```python
# æŸ¥çœ‹è§£ç å™¨ç¬¬0å±‚çš„äº¤å‰æ³¨æ„åŠ›
attn['decoder'][0]['cross_attention']
# å½¢çŠ¶: (batch_size, num_heads, decoder_len, encoder_len)

# å¯è§†åŒ–
visualizer.plot_attention_heatmap(
    attn['decoder'][0]['cross_attention'],
    title='è§£ç å™¨äº¤å‰æ³¨æ„åŠ›'
)
```

**å¦‚ä½•è§£è¯»**:

- **è¡Œ**: è§£ç å™¨ä½ç½®
- **åˆ—**: ç¼–ç å™¨ä½ç½®
- **å«ä¹‰**: è§£ç å™¨æ¯ä¸ªä½ç½®åœ¨å…³æ³¨ç¼–ç å™¨çš„å“ªäº›éƒ¨åˆ†

**ä¾‹å­**:

- å¦‚æœè§£ç å™¨ä½ç½®2åœ¨ç¼–ç å™¨ä½ç½®7å¾ˆäº® â†’ ç”Ÿæˆç¬¬2ä¸ªè¾“å‡ºæ—¶ä¸»è¦å‚è€ƒè¾“å…¥çš„ç¬¬7ä¸ªä½ç½®
- æŸè¡Œæœ‰å¤šä¸ªäº®ç‚¹ â†’ è¯¥è¾“å‡ºç»¼åˆå‚è€ƒäº†å¤šä¸ªè¾“å…¥ä½ç½®

### 3. è§£ç å™¨è‡ªæ³¨æ„åŠ› (Decoder Self-Attention)

**ä½œç”¨**: è§£ç å™¨å†…éƒ¨ï¼Œæ¯ä¸ªä½ç½®å…³æ³¨ä¹‹å‰çš„ä½ç½®ï¼ˆå› æœmaskï¼‰

```python
# æŸ¥çœ‹è§£ç å™¨ç¬¬0å±‚çš„è‡ªæ³¨æ„åŠ›
attn['decoder'][0]['self_attention']
# å½¢çŠ¶: (batch_size, num_heads, seq_len, seq_len)

# å¯è§†åŒ–
visualizer.plot_attention_heatmap(
    attn['decoder'][0]['self_attention'],
    title='è§£ç å™¨è‡ªæ³¨æ„åŠ›ï¼ˆå› æœï¼‰'
)
```

**å¦‚ä½•è§£è¯»**:

- **ä¸‹ä¸‰è§’çŸ©é˜µ**: åªèƒ½çœ‹åˆ°å½“å‰å’Œä¹‹å‰çš„ä½ç½®
- **ä¸Šä¸‰è§’å…¨é»‘**: å› æœmaskï¼Œä¸èƒ½çœ‹åˆ°æœªæ¥

## ğŸ” å…­ç§è§‚å¯Ÿæ–¹æ³•

### æ–¹æ³•1: åŸºç¡€çƒ­åŠ›å›¾ï¼ˆæœ€å¸¸ç”¨ï¼‰

```python
# æŸ¥çœ‹æŸä¸€å±‚æŸä¸€å¤´çš„æ³¨æ„åŠ›
visualizer.plot_attention_heatmap(
    attn['encoder'][0]['self_attention'],
    head_idx=3,      # ç¬¬3ä¸ªå¤´ï¼ŒNone=å¹³å‡æ‰€æœ‰å¤´
    batch_idx=0,     # ç¬¬0ä¸ªæ ·æœ¬
    save_path='attn.png'
)
```

**ç”¨é€”**: å¿«é€ŸæŸ¥çœ‹å•å±‚æ³¨æ„åŠ›åˆ†å¸ƒ

### æ–¹æ³•2: å±‚çº§å¯¹æ¯”

```python
# å¯¹æ¯”æ‰€æœ‰ç¼–ç å™¨å±‚
visualizer.plot_layer_comparison(
    attn['encoder'],
    attention_type='self_attention',
    title='ç¼–ç å™¨å„å±‚å¯¹æ¯”'
)

# å¯¹æ¯”æ‰€æœ‰è§£ç å™¨äº¤å‰æ³¨æ„åŠ›
visualizer.plot_layer_comparison(
    attn['decoder'],
    attention_type='cross_attention',
    title='è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å„å±‚å¯¹æ¯”'
)
```

**ç”¨é€”**: è§‚å¯Ÿæ³¨æ„åŠ›éšæ·±åº¦çš„å˜åŒ–

### æ–¹æ³•3: å¤šå¤´åˆ†æ

```python
# å¹¶æ’æ˜¾ç¤ºæ‰€æœ‰æ³¨æ„åŠ›å¤´
visualizer.plot_multi_head_attention(
    attn['encoder'][0]['self_attention'],
    title='ç¼–ç å™¨ç¬¬0å±‚æ‰€æœ‰å¤´'
)
```

**ç”¨é€”**: ç†è§£ä¸åŒå¤´å­¦åˆ°äº†ä»€ä¹ˆ

### æ–¹æ³•4: å®Œæ•´æ³¨æ„åŠ›æµ

```python
# ä»ç¼–ç å™¨åˆ°è§£ç å™¨çš„å®Œæ•´æµç¨‹
visualizer.plot_attention_flow(
    encoder_attention=attn['encoder'][0]['self_attention'],
    decoder_self_attention=attn['decoder'][0]['self_attention'],
    decoder_cross_attention=attn['decoder'][0]['cross_attention'],
    layer_idx=0
)
```

**ç”¨é€”**: ç†è§£ä¿¡æ¯æµåŠ¨è·¯å¾„

### æ–¹æ³•5: æ•°å€¼åˆ†æ

```python
# æŸ¥çœ‹ç‰¹å®šä½ç½®çš„æ³¨æ„åŠ›åˆ†å¸ƒ
decoder_pos = 5
cross_attn = attn['decoder'][0]['cross_attention']
attn_at_pos = cross_attn[0, :, decoder_pos, :].mean(dim=0)

# æ‰¾å‡ºæœ€å…³æ³¨çš„ä½ç½®
top_values, top_indices = torch.topk(attn_at_pos, 5)
print("è§£ç å™¨ä½ç½®5æœ€å…³æ³¨ç¼–ç å™¨çš„ä½ç½®:", top_indices)
```

**ç”¨é€”**: ç²¾ç¡®åˆ†ææŸä¸ªä½ç½®çš„æ³¨æ„åŠ›

### æ–¹æ³•6: ç»Ÿè®¡ä¿¡æ¯

```python
# ä¿å­˜æ‰€æœ‰å±‚çš„ç»Ÿè®¡æ•°æ®
visualizer.save_attention_statistics(
    attn,
    save_path='attention_stats.json'
)
```

**ç”¨é€”**: å®šé‡åˆ†æå’Œè®°å½•

## ğŸ“‹ æ³¨æ„åŠ›æ•°æ®ç»“æ„

```python
attention_weights = {
    'encoder': [
        {'self_attention': Tensor(B, NH, T, T)},    # Layer 0
        {'self_attention': Tensor(B, NH, T, T)},    # Layer 1
        # ...
    ],
    'decoder': [
        {
            'self_attention': Tensor(B, NH, T, T),
            'cross_attention': Tensor(B, NH, T_dec, T_enc)
        },  # Layer 0
        # ...
    ]
}
```

**ç»´åº¦è¯´æ˜**:

- `B`: Batch Sizeï¼ˆæ‰¹æ¬¡å¤§å°ï¼‰
- `NH`: Number of Headsï¼ˆæ³¨æ„åŠ›å¤´æ•°ï¼‰
- `T`: Sequence Lengthï¼ˆåºåˆ—é•¿åº¦ï¼‰
- `T_dec`: Decoderåºåˆ—é•¿åº¦
- `T_enc`: Encoderåºåˆ—é•¿åº¦

## ğŸ¨ å®é™…åº”ç”¨åœºæ™¯

### åœºæ™¯1: è°ƒè¯•æ¨¡å‹è¡Œä¸º

```python
# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­¦åˆ°äº†åˆç†çš„æ³¨æ„åŠ›æ¨¡å¼
model.enable_attention_storage()
with torch.no_grad():
    output, attn = model(enc, dec, return_attention_weights=True)

# æŸ¥çœ‹è§£ç å™¨æ˜¯å¦æ­£ç¡®å…³æ³¨ç¼–ç å™¨
visualizer.plot_attention_heatmap(
    attn['decoder'][0]['cross_attention'],
    title='è§£ç å™¨å…³æ³¨è¾“å…¥çš„å“ªäº›ä½ç½®ï¼Ÿ'
)
```

### åœºæ™¯2: åˆ†ææ¨¡å‹å­¦ä¹ è¿‡ç¨‹

```python
# è®­ç»ƒä¸åŒé˜¶æ®µçš„æ³¨æ„åŠ›å¯¹æ¯”
for epoch in [0, 10, 50, 100]:
    load_checkpoint(f'epoch_{epoch}.pth')
    _, attn = model(enc, dec, return_attention_weights=True)
    
    visualizer.plot_attention_heatmap(
        attn['decoder'][0]['cross_attention'],
        save_path=f'attention_epoch_{epoch}.png'
    )
```

### åœºæ™¯3: å¯è§£é‡Šæ€§åˆ†æ

```python
# è§£é‡Šæ¨¡å‹çš„é¢„æµ‹
# ä¾‹å¦‚ï¼šä¸ºä»€ä¹ˆæ¨¡å‹ç”Ÿæˆäº†è¿™ä¸ªè¾“å‡ºï¼Ÿ
output, attn = model(enc, dec, return_attention_weights=True)

# æŸ¥çœ‹ç”ŸæˆæŸä¸ªtokenæ—¶å…³æ³¨äº†è¾“å…¥çš„å“ªäº›éƒ¨åˆ†
output_pos = 10  # ç¬¬10ä¸ªè¾“å‡ºtoken
cross_attn = attn['decoder'][-1]['cross_attention'][0, :, output_pos, :]

# å¯è§†åŒ–ï¼šç”Ÿæˆç¬¬10ä¸ªtokenæ—¶çš„æ³¨æ„åŠ›åˆ†å¸ƒ
plt.bar(range(len(cross_attn.mean(0))), cross_attn.mean(0).cpu())
plt.title(f'ç”Ÿæˆç¬¬{output_pos}ä¸ªtokenæ—¶çš„è¾“å…¥æ³¨æ„åŠ›')
plt.xlabel('è¾“å…¥ä½ç½®')
plt.ylabel('æ³¨æ„åŠ›æƒé‡')
```

## ğŸ’¡ å¸¸è§æ¨¡å¼è§£è¯»

### æ¨¡å¼1: å¯¹è§’çº¿æ˜æ˜¾

```
â–  â–¡ â–¡ â–¡ â–¡
â–¡ â–  â–¡ â–¡ â–¡
â–¡ â–¡ â–  â–¡ â–¡
â–¡ â–¡ â–¡ â–  â–¡
â–¡ â–¡ â–¡ â–¡ â– 
```

**å«ä¹‰**: ä½ç½®ä¸»è¦å…³æ³¨è‡ªå·±ï¼Œå¯èƒ½æ˜¯æ¨¡å‹ä¾èµ–å±€éƒ¨ä¿¡æ¯

### æ¨¡å¼2: å…¨å±€æ³¨æ„

```
â–  â–  â–  â–  â– 
â–  â–  â–  â–  â– 
â–  â–  â–  â–  â– 
â–  â–  â–  â–  â– 
â–  â–  â–  â–  â– 
```

**å«ä¹‰**: æ¯ä¸ªä½ç½®å…³æ³¨æ‰€æœ‰ä½ç½®ï¼Œæ•æ‰å…¨å±€ä¾èµ–

### æ¨¡å¼3: ç¨€ç–æ³¨æ„

```
â–  â–¡ â–¡ â–  â–¡
â–¡ â–¡ â–  â–¡ â–¡
â–  â–¡ â–¡ â–¡ â– 
â–¡ â–  â–¡ â–¡ â–¡
â–¡ â–¡ â–  â–¡ â– 
```

**å«ä¹‰**: é€‰æ‹©æ€§å…³æ³¨ï¼Œå­¦åˆ°äº†ç‰¹å®šçš„ä¾èµ–å…³ç³»

### æ¨¡å¼4: å› æœä¸‹ä¸‰è§’ï¼ˆè§£ç å™¨ï¼‰

```
â–  â–¡ â–¡ â–¡ â–¡
â–  â–  â–¡ â–¡ â–¡
â–  â–  â–  â–¡ â–¡
â–  â–  â–  â–  â–¡
â–  â–  â–  â–  â– 
```

**å«ä¹‰**: æ­£å¸¸çš„å› æœæ³¨æ„åŠ›ï¼Œåªçœ‹å½“å‰å’Œä¹‹å‰

## ğŸš€ å¿«é€Ÿè¿è¡Œå®Œæ•´ç¤ºä¾‹

```bash
# è¿è¡ŒåŒ…å«æ‰€æœ‰å¯è§†åŒ–æ–¹æ³•çš„ç¤ºä¾‹
python agent/utils/how_to_visualize_attention.py
```

è¿™å°†ç”Ÿæˆï¼š

1. `encoder_layer0_self_attention.png` - ç¼–ç å™¨è‡ªæ³¨æ„åŠ›
2. `decoder_layer0_cross_attention.png` - è§£ç å™¨äº¤å‰æ³¨æ„åŠ›
3. `decoder_layer0_self_attention.png` - è§£ç å™¨è‡ªæ³¨æ„åŠ›
4. `encoder_all_layers_comparison.png` - ç¼–ç å™¨å±‚çº§å¯¹æ¯”
5. `decoder_all_layers_cross_comparison.png` - è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å¯¹æ¯”
6. `encoder_layer0_all_heads.png` - å¤šå¤´å¯¹æ¯”
7. `attention_flow_layer0.png` - å®Œæ•´æ³¨æ„åŠ›æµ
8. `attention_statistics.json` - ç»Ÿè®¡ä¿¡æ¯

## â“ å¸¸è§é—®é¢˜

### Q1: æ³¨æ„åŠ›æƒé‡ä¸ºNoneï¼Ÿ

```python
# ç¡®ä¿å¯ç”¨äº†æ³¨æ„åŠ›å­˜å‚¨
model.enable_attention_storage()
_, attn = model(enc, dec, return_attention_weights=True)
```

### Q2: å¦‚ä½•åªçœ‹ç‰¹å®šçš„å¤´ï¼Ÿ

```python
# head_idx=3 åªçœ‹ç¬¬3ä¸ªå¤´
visualizer.plot_attention_heatmap(
    attn['encoder'][0]['self_attention'],
    head_idx=3
)
```

### Q3: å¦‚ä½•æ¯”è¾ƒä¸åŒæ ·æœ¬ï¼Ÿ

```python
# batch_idxæŒ‡å®šè¦çœ‹ç¬¬å‡ ä¸ªæ ·æœ¬
for i in range(batch_size):
    visualizer.plot_attention_heatmap(
        attn['encoder'][0]['self_attention'],
        batch_idx=i,
        save_path=f'sample_{i}.png'
    )
```

### Q4: å†…å­˜ä¸è¶³ï¼Ÿ

```python
# åªå¯è§†åŒ–å…³é”®å±‚
model.enable_attention_storage()
_, attn = model(enc, dec, return_attention_weights=True)
model.disable_attention_storage()  # ç«‹å³ç¦ç”¨

# åªä¿å­˜éœ€è¦çš„å±‚
visualizer.plot_attention_heatmap(
    attn['decoder'][0]['cross_attention'],  # åªçœ‹ç¬¬0å±‚
    show=False  # ä¸æ˜¾ç¤ºï¼Œåªä¿å­˜
)
```

## ğŸ“š æ›´å¤šèµ„æº

- è¯¦ç»†æŒ‡å—: `agent/utils/ATTENTION_VISUALIZATION_GUIDE.md`
- å®Œæ•´ç¤ºä¾‹: `agent/utils/attention_visualization_example.py`
- è®­ç»ƒç›‘æ§: `agent/utils/TRAINING_ATTENTION_GUIDE.md`
