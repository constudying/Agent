# è®­ç»ƒæ—¶æ³¨æ„åŠ›ç›‘æ§ - å¿«é€Ÿå‚è€ƒ

## ä¸€è¡Œä»£ç é›†æˆ

```python
from agent.utils.training_attention_monitor import TrainingAttentionMonitor

# åˆ›å»ºç›‘æ§å™¨
monitor = TrainingAttentionMonitor(save_dir='./attn_logs', save_frequency=100)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ ä¸€è¡Œ
for step, batch in enumerate(dataloader):
    output = model(enc, dec)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
    
    monitor.log_attention(model, enc, dec, step, loss.item())  # æ·»åŠ è¿™è¡Œ

# è®­ç»ƒç»“æŸ
monitor.close()
```

## æ ¸å¿ƒç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| ğŸš€ **ä¸å½±å“è®­ç»ƒ** | å¼‚æ­¥åå°å¤„ç†ï¼Œä¸»çº¿ç¨‹ä¸é˜»å¡ |
| ğŸ’¾ **è‡ªåŠ¨ç®¡ç†å†…å­˜** | ç«‹å³é‡Šæ”¾GPUå†…å­˜ï¼Œé˜²æ­¢OOM |
| ğŸ“Š **TensorBoardé›†æˆ** | å®æ—¶æŸ¥çœ‹æ³¨æ„åŠ›ç»Ÿè®¡ |
| âš™ï¸ **é«˜åº¦å¯é…ç½®** | æ§åˆ¶é¢‘ç‡ã€å±‚ã€å¤´ã€å¯è§†åŒ–ç±»å‹ |
| ğŸ¯ **ä¸¤ç§æ¨¡å¼** | å®Œæ•´å¯è§†åŒ– æˆ– è½»é‡çº§ç»Ÿè®¡ |

## ä¸¤ç§ç›‘æ§å™¨å¯¹æ¯”

| ç‰¹æ€§ | TrainingAttentionMonitor | LightweightAttentionMonitor |
|------|-------------------------|---------------------------|
| ç”Ÿæˆå›¾åƒ | âœ… æ˜¯ | âŒ å¦ |
| ä¿å­˜ç»Ÿè®¡ | âœ… æ˜¯ | âœ… æ˜¯ |
| é€Ÿåº¦å½±å“ | ~2% | ~0.5% |
| ç£ç›˜å ç”¨ | ä¸­ç­‰ | å¾ˆå° |
| é€‚ç”¨åœºæ™¯ | éªŒè¯ã€æ£€æŸ¥ç‚¹ | é•¿æ—¶é—´è®­ç»ƒ |

## å¸¸ç”¨é…ç½®

### ğŸ¯ æ¨èé…ç½®ï¼ˆå¹³è¡¡ï¼‰

```python
monitor = TrainingAttentionMonitor(
    save_dir='./attention_logs',
    save_frequency=100,              # æ¯100æ­¥
    visualization_types=['heatmap', 'statistics'],
    layers_to_visualize=[0, -1],    # é¦–å°¾å±‚
    use_tensorboard=True
)
```

### ğŸ” è°ƒè¯•é…ç½®ï¼ˆè¯¦ç»†ï¼‰

```python
monitor = TrainingAttentionMonitor(
    save_frequency=10,               # æ›´é¢‘ç¹
    visualization_types=['heatmap', 'multi_head', 'layer_comparison', 'statistics'],
    layers_to_visualize=None,       # æ‰€æœ‰å±‚
)
```

### âš¡ é«˜æ€§èƒ½é…ç½®ï¼ˆæœ€å¿«ï¼‰

```python
monitor = LightweightAttentionMonitor(
    save_frequency=50,
    use_tensorboard=True
)
```

## æ€§èƒ½å½±å“

```
åŸºå‡†è®­ç»ƒé€Ÿåº¦: 100%

+ LightweightAttentionMonitor:  100.5%  (+0.5%)
+ TrainingAttentionMonitor:     102%    (+2%)
```

## æŸ¥çœ‹ç»“æœ

### æ–‡ä»¶ç³»ç»Ÿ

```bash
attention_logs/
â”œâ”€â”€ step_100/
â”‚   â”œâ”€â”€ encoder_layer0_self_attn.png      # æ³¨æ„åŠ›çƒ­åŠ›å›¾
â”‚   â”œâ”€â”€ decoder_layer0_cross_attn.png     # äº¤å‰æ³¨æ„åŠ›
â”‚   â””â”€â”€ statistics.json                    # ç»Ÿè®¡æ•°æ®
â”œâ”€â”€ step_200/
â””â”€â”€ tensorboard/                           # TensorBoardæ—¥å¿—
```

### TensorBoard

```bash
tensorboard --logdir=./attention_logs/tensorboard
# è®¿é—® http://localhost:6006
```

## å¸¸è§é—®é¢˜é€ŸæŸ¥

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|---------|
| è®­ç»ƒå˜æ…¢ | æé«˜`save_frequency`æˆ–ä½¿ç”¨`LightweightAttentionMonitor` |
| å†…å­˜å ç”¨é«˜ | è®¾ç½®`save_raw_weights=False`ï¼Œ`max_workers=1` |
| ç£ç›˜å æ»¡ | ä½¿ç”¨`LightweightAttentionMonitor`æˆ–åªä¿å­˜`statistics` |
| GPUå†…å­˜ä¸è¶³ | å·²è‡ªåŠ¨å¤„ç†ï¼Œæ³¨æ„åŠ›æƒé‡ä¼šç«‹å³ç§»åˆ°CPU |

## å®Œæ•´ç¤ºä¾‹

```python
from agent.utils.training_attention_monitor import TrainingAttentionMonitor
import torch.nn as nn
import torch.optim as optim

# æ¨¡å‹å’Œä¼˜åŒ–å™¨
model = Transformer(...)
optimizer = optim.Adam(model.parameters())
criterion = nn.MSELoss()

# åˆ›å»ºç›‘æ§å™¨
with TrainingAttentionMonitor(
    save_dir='./attention_logs',
    save_frequency=100,
    use_tensorboard=True
) as monitor:
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        for step, batch in enumerate(dataloader):
            # å‡†å¤‡æ•°æ®
            enc, dec, target = prepare_batch(batch)
            
            # å‰å‘ä¼ æ’­
            output = model(enc, dec)
            loss = criterion(output, target)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # è®°å½•æ³¨æ„åŠ›ï¼ˆä¸å½±å“è®­ç»ƒï¼‰
            monitor.log_attention(
                model, enc, dec, 
                global_step, loss.item(), epoch
            )

# è‡ªåŠ¨å…³é—­å’Œæ¸…ç†
```

## æ›´å¤šä¿¡æ¯

- è¯¦ç»†æ–‡æ¡£: `agent/utils/TRAINING_ATTENTION_GUIDE.md`
- å®Œæ•´ç¤ºä¾‹: `agent/utils/training_with_attention_example.py`
- è¿è¡Œç¤ºä¾‹: `python agent/utils/training_with_attention_example.py`

## å…³é”®è¦ç‚¹

1. âœ… ä½¿ç”¨å¼‚æ­¥å¤„ç†ï¼Œè®­ç»ƒä¸ä¼šè¢«é˜»å¡
2. âœ… è‡ªåŠ¨é‡Šæ”¾GPUå†…å­˜ï¼Œé˜²æ­¢OOM
3. âœ… æ ¹æ®éœ€æ±‚é€‰æ‹©å®Œæ•´æˆ–è½»é‡çº§æ¨¡å¼
4. âœ… ä½¿ç”¨TensorBoardå®æ—¶ç›‘æ§
5. âœ… è®°å¾—åœ¨è®­ç»ƒç»“æŸæ—¶è°ƒç”¨`close()`
