# Transformeræ³¨æ„åŠ›å¯è§†åŒ–åŠŸèƒ½æ€»ç»“

## å®ç°çš„åŠŸèƒ½

### 1. æ ¸å¿ƒä¿®æ”¹

#### æ³¨æ„åŠ›æ¨¡å—å¢å¼º

- **NormalSelfAttention**: æ·»åŠ æ³¨æ„åŠ›æƒé‡å­˜å‚¨å’Œè¿”å›åŠŸèƒ½
- **CausalSelfAttention**: æ·»åŠ æ³¨æ„åŠ›æƒé‡å­˜å‚¨å’Œè¿”å›åŠŸèƒ½  
- **CrossAttention**: æ·»åŠ æ³¨æ„åŠ›æƒé‡å­˜å‚¨å’Œè¿”å›åŠŸèƒ½

æ¯ä¸ªæ¨¡å—æ–°å¢ï¼š

- `store_attention` å‚æ•°ï¼šæ§åˆ¶æ˜¯å¦è‡ªåŠ¨å­˜å‚¨æ³¨æ„åŠ›æƒé‡
- `attention_weights` å±æ€§ï¼šå­˜å‚¨æ³¨æ„åŠ›æƒé‡
- `return_attention` å‚æ•°ï¼šæ§åˆ¶forwardæ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡

#### Transformerç±»å¢å¼º

æ–°å¢æ–¹æ³•ï¼š

```python
# å¯ç”¨æ‰€æœ‰å±‚çš„æ³¨æ„åŠ›å­˜å‚¨
model.enable_attention_storage()

# ç¦ç”¨æ³¨æ„åŠ›å­˜å‚¨ï¼ˆè®­ç»ƒæ—¶èŠ‚çœå†…å­˜ï¼‰
model.disable_attention_storage()

# æ”¶é›†æ‰€æœ‰å±‚çš„æ³¨æ„åŠ›æƒé‡
attention_weights = model.get_attention_weights()

# å‰å‘ä¼ æ’­æ—¶è¿”å›æ³¨æ„åŠ›æƒé‡
output, attention_weights = model(enc, dec, return_attention_weights=True)
```

### 2. AttentionVisualizer å¯è§†åŒ–å·¥å…·ç±»

æä¾›5ç§ä¸»è¦å¯è§†åŒ–æ–¹æ³•ï¼š

#### 2.1 åŸºç¡€çƒ­åŠ›å›¾

```python
visualizer.plot_attention_heatmap(
    attention_weights['encoder'][0]['self_attention'],
    head_idx=3,  # ç‰¹å®šå¤´ï¼ŒNoneè¡¨ç¤ºå¹³å‡æ‰€æœ‰å¤´
    title='Encoder Layer 0 Self-Attention'
)
```

#### 2.2 å¤šå¤´æ³¨æ„åŠ›å¯¹æ¯”

```python
visualizer.plot_multi_head_attention(
    attention_weights['encoder'][0]['self_attention'],
    title='All Attention Heads'
)
```

#### 2.3 å±‚çº§å¯¹æ¯”

```python
visualizer.plot_layer_comparison(
    attention_weights['encoder'],
    attention_type='self_attention',
    title='Encoder Layers Comparison'
)
```

#### 2.4 å®Œæ•´æ³¨æ„åŠ›æµ

```python
visualizer.plot_attention_flow(
    encoder_attention=...,
    decoder_self_attention=...,
    decoder_cross_attention=...,
    layer_idx=0
)
```

#### 2.5 ç»Ÿè®¡ä¿¡æ¯ä¿å­˜

```python
visualizer.save_attention_statistics(
    attention_weights,
    save_path='stats.json'
)
```

## ä½¿ç”¨æµç¨‹

```python
# 1. åˆ›å»ºæ¨¡å‹
model = Transformer(...)

# 2. å¯ç”¨æ³¨æ„åŠ›å­˜å‚¨
model.enable_attention_storage()

# 3. å‰å‘ä¼ æ’­å¹¶è·å–æ³¨æ„åŠ›
output, attn = model(enc, dec, return_attention_weights=True)

# 4. å¯è§†åŒ–
visualizer = AttentionVisualizer()
visualizer.plot_attention_heatmap(
    attn['encoder'][0]['self_attention'],
    save_path='attention.png'
)

# 5. è®­ç»ƒæ—¶è®°å¾—ç¦ç”¨
model.disable_attention_storage()
```

## æ³¨æ„åŠ›æƒé‡æ•°æ®ç»“æ„

```python
attention_weights = {
    'encoder': [
        {'self_attention': Tensor(B, NH, T, T)},  # Layer 0
        {'self_attention': Tensor(B, NH, T, T)},  # Layer 1
        ...
    ],
    'decoder': [
        {
            'self_attention': Tensor(B, NH, T, T),
            'cross_attention': Tensor(B, NH, T, S)
        },  # Layer 0
        ...
    ]
}
```

å…¶ä¸­ï¼š

- B: Batch Size
- NH: Number of Heads
- T: Decoder Sequence Length
- S: Encoder Sequence Length

## æ–‡ä»¶æ¸…å•

1. **ä¸»è¦ä»£ç **: `/home/lsy/cjh/project1/Agent/agent/models/transformer.py`
   - ä¿®æ”¹çš„æ³¨æ„åŠ›æ¨¡å—
   - ä¿®æ”¹çš„Transformerç±»
   - æ–°å¢çš„AttentionVisualizerç±»

2. **ä½¿ç”¨ç¤ºä¾‹**: `/home/lsy/cjh/project1/Agent/agent/utils/attention_visualization_example.py`
   - 5ä¸ªå®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹
   - æ¶µç›–æ‰€æœ‰å¯è§†åŒ–åŠŸèƒ½

3. **è¯¦ç»†æ–‡æ¡£**: `/home/lsy/cjh/project1/Agent/agent/utils/ATTENTION_VISUALIZATION_GUIDE.md`
   - å®Œæ•´çš„ä½¿ç”¨æŒ‡å—
   - APIå‚è€ƒ
   - é«˜çº§ç”¨æ³•å’Œæ•…éšœæ’é™¤

## åº”ç”¨åœºæ™¯

1. **æ¨¡å‹åˆ†æ**ï¼šç†è§£æ¨¡å‹å…³æ³¨å“ªäº›è¾“å…¥éƒ¨åˆ†
2. **è°ƒè¯•**ï¼šè¯Šæ–­æ¨¡å‹è¡Œä¸ºå¼‚å¸¸
3. **ç ”ç©¶**ï¼šåˆ†æä¸åŒå±‚å’Œå¤´çš„æ³¨æ„åŠ›æ¨¡å¼
4. **å¯è§£é‡Šæ€§**ï¼šå‘ç”¨æˆ·å±•ç¤ºæ¨¡å‹å†³ç­–ä¾æ®
5. **è®ºæ–‡å¯è§†åŒ–**ï¼šç”Ÿæˆé«˜è´¨é‡çš„æ³¨æ„åŠ›å›¾è¡¨

## æ€§èƒ½å»ºè®®

- âœ… **æ¨ç†/éªŒè¯æ—¶**: å¯ç”¨æ³¨æ„åŠ›å­˜å‚¨
- âŒ **è®­ç»ƒæ—¶**: ç¦ç”¨æ³¨æ„åŠ›å­˜å‚¨ä»¥èŠ‚çœå†…å­˜
- ğŸ’¡ **å¤§æ‰¹æ¬¡**: åªå¯è§†åŒ–batch_idx=0
- ğŸ’¡ **å¤šå±‚æ¨¡å‹**: é€‰æ‹©æ€§å¯è§†åŒ–å…³é”®å±‚

## ä¾èµ–è¦æ±‚

```bash
pip install matplotlib seaborn
```

## å¿«é€Ÿæµ‹è¯•

```bash
python agent/utils/attention_visualization_example.py
```

è¿™å°†ç”Ÿæˆå¤šä¸ªç¤ºä¾‹å¯è§†åŒ–å›¾ç‰‡å’Œç»Ÿè®¡æ–‡ä»¶ã€‚
